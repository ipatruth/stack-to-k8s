# Lab 8.5: Multi-Tenancy Patterns 🏢

**⏱ Time**: 60 minutes  
**🎯 Difficulty**: ⭐⭐⭐⭐ Expert  
**📋 Prerequisites**: Labs 5 (RBAC), 8 (Chaos Engineering)

---

## 🎯 Objective

**The Problem**: Your company has 3 teams sharing one Kubernetes cluster. Without proper isolation:
- Team A accidentally deletes Team B's database (oops!)
- Team C's runaway pod consumes all cluster CPU (everyone else crashes)
- No one knows which team is driving the $10K/month cloud bill

**This Lab**: Implement production-grade multi-tenancy with:
1. **Namespace isolation** (network policies + RBAC)
2. **Resource quotas** (CPU, memory, PVC limits per team)
3. **LimitRanges** (default requests/limits, min/max constraints)
4. **Cost allocation** (label-based billing)
5. **Tenant onboarding automation** (GitOps-based)

**Why This Matters**: Shopify runs **100+ teams on shared clusters** with zero cross-team incidents. This lab teaches their patterns.

---

## 📚 Key Concepts

### Multi-Tenancy Models

| Model | Description | Use Case |
|-------|-------------|----------|
| **Soft Multi-Tenancy** | Namespace isolation (NetworkPolicy + RBAC) | Internal teams (trust assumed) |
| **Hard Multi-Tenancy** | Node isolation (taints + tolerations) | External customers (zero trust) |
| **Cluster-per-Tenant** | Separate cluster per tenant | High-security (banks, healthcare) |

**This Lab**: Soft multi-tenancy (most common, cost-effective).

### The Tenant Hierarchy

```
Cluster
├── Namespace: team-a-prod
│   ├── ResourceQuota: cpu=4, memory=8Gi, pods=50
│   ├── LimitRange: default requests/limits
│   └── NetworkPolicy: allow-same-namespace + allow-egress
├── Namespace: team-b-prod
│   ├── ResourceQuota: cpu=8, memory=16Gi, pods=100
│   ├── LimitRange: ...
│   └── NetworkPolicy: ...
└── Namespace: team-c-dev
    ├── ResourceQuota: cpu=2, memory=4Gi, pods=20
    └── ...
```

---

## 🔧 Setup: Create Three Tenant Namespaces

```bash
# Create namespaces for 3 teams
kubectl create namespace team-a-prod
kubectl create namespace team-b-prod
kubectl create namespace team-c-dev

# Label namespaces for cost allocation
kubectl label namespace team-a-prod team=team-a environment=production cost-center=engineering
kubectl label namespace team-b-prod team=team-b environment=production cost-center=data-science
kubectl label namespace team-c-dev team=team-c environment=development cost-center=engineering

# Verify
kubectl get namespaces --show-labels
```

---

## 📊 Part 1: Resource Quotas (Prevent Resource Hogging)

### What ResourceQuota Controls

| Resource | Quota Key | Example Value |
|----------|-----------|---------------|
| **CPU** | `requests.cpu` / `limits.cpu` | `4` (4 vCPUs) |
| **Memory** | `requests.memory` / `limits.memory` | `8Gi` |
| **Pods** | `count/pods` | `50` |
| **PVCs** | `persistentvolumeclaims` | `10` |
| **Services** | `count/services.loadbalancers` | `3` |

### Step 1: Create ResourceQuota for Team A (Production)

```yaml
kubectl apply -f - <<EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-quota
  namespace: team-a-prod
spec:
  hard:
    # Compute quotas
    requests.cpu: "4"          # Total CPU requests: 4 vCPUs
    requests.memory: 8Gi       # Total memory requests: 8 GB
    limits.cpu: "8"            # Total CPU limits: 8 vCPUs
    limits.memory: 16Gi        # Total memory limits: 16 GB
    
    # Object count quotas
    count/pods: "50"           # Max 50 pods
    count/services: "10"       # Max 10 services
    count/services.loadbalancers: "2"  # Max 2 LoadBalancer services
    
    # Storage quotas
    persistentvolumeclaims: "10"       # Max 10 PVCs
    requests.storage: 100Gi            # Total PVC storage: 100 GB
EOF

# Verify quota
kubectl describe resourcequota team-a-quota -n team-a-prod
# Expected: Used: 0/4 CPU, 0/8Gi memory (nothing deployed yet)
```

### Step 2: Create Smaller Quota for Team C (Development)

```yaml
kubectl apply -f - <<EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-c-quota
  namespace: team-c-dev
spec:
  hard:
    requests.cpu: "2"          # 50% less than production
    requests.memory: 4Gi
    limits.cpu: "4"
    limits.memory: 8Gi
    count/pods: "20"           # Fewer pods (dev environment)
    count/services.loadbalancers: "0"  # No LoadBalancers in dev!
    persistentvolumeclaims: "5"
    requests.storage: 50Gi
EOF
```

### Step 3: Test Quota Enforcement

```bash
# Try to create pod WITHOUT resource requests (should fail)
kubectl run nginx --image=nginx -n team-a-prod
# ❌ Error: pods "nginx" is forbidden: failed quota: team-a-quota: must specify requests.cpu, requests.memory

# Create pod WITH resource requests (should succeed)
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: team-a-prod
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      requests:
        cpu: 100m      # 0.1 vCPU
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
EOF

# Check quota usage
kubectl describe resourcequota team-a-quota -n team-a-prod
# Expected: Used: 100m/4 CPU, 128Mi/8Gi memory (quota consumed)
```

### Step 4: Trigger Quota Exceeded

```bash
# Try to create 60 pods (quota allows max 50)
kubectl create deployment nginx-flood --image=nginx --replicas=60 -n team-a-prod

# Check replica set status
kubectl get rs -n team-a-prod
# Expected: DESIRED=60, CURRENT=50 (quota blocks extra pods)

# Check events
kubectl get events -n team-a-prod --sort-by='.lastTimestamp' | grep quota
# Expected: "exceeded quota: team-a-quota, requested: count/pods=1, used: count/pods=50, limited: count/pods=50"

# Cleanup
kubectl delete deployment nginx-flood -n team-a-prod
```

**✅ Success!** ResourceQuota prevents one team from consuming entire cluster.

---

## 📏 Part 2: LimitRanges (Default Requests/Limits)

### Problem Without LimitRanges

```yaml
# Developer forgets to set resource requests
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: app
    image: myapp
    # ❌ Missing: resources.requests/limits
```

**Result**: Pod gets scheduled with **0 requests** (can be evicted anytime) or **no limits** (can consume entire node).

### Solution: LimitRange

```yaml
kubectl apply -f - <<EOF
apiVersion: v1
kind: LimitRange
metadata:
  name: team-a-limits
  namespace: team-a-prod
spec:
  limits:
  # Container defaults
  - type: Container
    default:  # Default limits (if not specified)
      cpu: 500m
      memory: 512Mi
    defaultRequest:  # Default requests (if not specified)
      cpu: 100m
      memory: 128Mi
    max:  # Maximum allowed
      cpu: 2
      memory: 4Gi
    min:  # Minimum required
      cpu: 50m
      memory: 64Mi
  
  # Pod limits (sum of all containers)
  - type: Pod
    max:
      cpu: 4
      memory: 8Gi
  
  # PVC limits
  - type: PersistentVolumeClaim
    max:
      storage: 50Gi
    min:
      storage: 1Gi
EOF

# Verify
kubectl describe limitrange team-a-limits -n team-a-prod
```

### Test LimitRange

```bash
# Create pod WITHOUT resource requests (LimitRange fills defaults)
kubectl run autopilot --image=nginx -n team-a-prod

# Check pod resources
kubectl get pod autopilot -n team-a-prod -o yaml | grep -A 10 resources:
# Expected:
# requests:
#   cpu: 100m       ← LimitRange default!
#   memory: 128Mi
# limits:
#   cpu: 500m
#   memory: 512Mi

# Try to create pod ABOVE max limits (should fail)
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: greedy-pod
  namespace: team-a-prod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: 3  # Exceeds LimitRange max (2 CPU)
      limits:
        cpu: 3
EOF
# ❌ Error: Pod "greedy-pod" is invalid: spec.containers[0].resources.requests: Invalid value: "3": must be less than or equal to cpu limit

# Cleanup
kubectl delete pod autopilot greedy-pod -n team-a-prod
```

**✅ Success!** LimitRange enforces sensible defaults + prevents resource abuse.

---

## 🔒 Part 3: Network Isolation (NetworkPolicy)

### Default Allow-All (Insecure)

Without NetworkPolicy, **any pod can talk to any pod**:
```
team-a-prod/app → team-b-prod/database ❌ (cross-team access)
team-c-dev/test → team-a-prod/payment-api ❌ (dev accesses prod)
```

### Step 1: Deny All Ingress (Whitelist Approach)

```yaml
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: team-a-prod
spec:
  podSelector: {}  # Applies to all pods in namespace
  policyTypes:
  - Ingress
  # No ingress rules = deny all
EOF

# Test: Try to access Team A's pod from Team B's namespace (should fail)
kubectl run curl-test --image=curlimages/curl -n team-b-prod --rm -it -- sh -c "curl http://nginx.team-a-prod.svc.cluster.local --max-time 5"
# ❌ Expected: Timeout (NetworkPolicy blocked)
```

### Step 2: Allow Same-Namespace Traffic

```yaml
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: team-a-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}  # Allow from any pod in same namespace
EOF

# Test: Access from same namespace (should work)
kubectl run curl-test --image=curlimages/curl -n team-a-prod --rm -it -- sh -c "curl http://nginx.team-a-prod.svc.cluster.local --max-time 5"
# ✅ Expected: <h1>Welcome to nginx!</h1>
```

### Step 3: Allow Egress to Internet (Block Internal)

```yaml
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-internet
  namespace: team-a-prod
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  # Allow DNS (CoreDNS)
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53
  
  # Allow HTTPS to internet (not internal namespaces)
  - to:
    - namespaceSelector: {}  # All namespaces
    ports:
    - protocol: TCP
      port: 443
  
  # Block all other egress (no pod-to-pod across namespaces)
EOF
```

### Step 4: Production-Grade Namespace Isolation

```yaml
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: production-isolation
  namespace: team-a-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  
  # Ingress: Allow only from same namespace + ingress-nginx
  ingress:
  - from:
    - podSelector: {}  # Same namespace
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx  # Ingress controller
  
  # Egress: Allow DNS + external internet only
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53  # DNS
  - to:  # External traffic (not 10.0.0.0/8 internal)
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8      # Block internal pod IPs
        - 172.16.0.0/12   # Block internal service IPs
        - 192.168.0.0/16  # Block node IPs
EOF
```

**✅ Success!** Teams cannot accidentally access each other's pods.

---

## 👤 Part 4: RBAC (Who Can Do What)

### Step 1: Create Team-Specific ServiceAccounts

```bash
# Create ServiceAccounts for each team
kubectl create serviceaccount team-a-admin -n team-a-prod
kubectl create serviceaccount team-a-developer -n team-a-prod
kubectl create serviceaccount team-b-admin -n team-b-prod
```

### Step 2: Create Role (Namespace-Scoped Permissions)

```yaml
kubectl apply -f - <<EOF
# Admin role (full access to namespace)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: team-admin
  namespace: team-a-prod
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
# Developer role (read-only + pod logs)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: team-developer
  namespace: team-a-prod
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
EOF
```

### Step 3: Bind Roles to ServiceAccounts

```yaml
kubectl apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: team-a-admin-binding
  namespace: team-a-prod
subjects:
- kind: ServiceAccount
  name: team-a-admin
  namespace: team-a-prod
roleRef:
  kind: Role
  name: team-admin
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: team-a-developer-binding
  namespace: team-a-prod
subjects:
- kind: ServiceAccount
  name: team-a-developer
  namespace: team-a-prod
roleRef:
  kind: Role
  name: team-developer
  apiGroup: rbac.authorization.k8s.io
EOF
```

### Step 4: Test RBAC

```bash
# Test admin permissions (should work)
kubectl auth can-i delete pods -n team-a-prod --as=system:serviceaccount:team-a-prod:team-a-admin
# ✅ yes

# Test developer permissions (should fail)
kubectl auth can-i delete pods -n team-a-prod --as=system:serviceaccount:team-a-prod:team-a-developer
# ❌ no

# Test cross-namespace access (should fail)
kubectl auth can-i get pods -n team-b-prod --as=system:serviceaccount:team-a-prod:team-a-admin
# ❌ no
```

**✅ Success!** Developers can't delete production pods, teams can't access each other's namespaces.

---

## 💰 Part 5: Cost Allocation (Who's Spending What?)

### Step 1: Label Pods with Cost Metadata

```yaml
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: team-a-prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
        team: team-a                # Cost allocation label
        cost-center: engineering    # Billing department
        environment: production     # Prod costs more than dev
    spec:
      containers:
      - name: backend
        image: nginx
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
EOF
```

### Step 2: Query Cost by Team (Prometheus Example)

```promql
# Total CPU requests by team
sum(kube_pod_container_resource_requests{resource="cpu"}) by (namespace, label_team)

# Total memory requests by team
sum(kube_pod_container_resource_requests{resource="memory"}) by (namespace, label_team)

# Total pod count by team
count(kube_pod_info) by (namespace, label_team)
```

### Step 3: Generate Cost Report

```bash
# Get total CPU/memory requests per namespace
kubectl get pods -A -o json | jq -r '
  .items[] |
  select(.metadata.labels.team != null) |
  {
    team: .metadata.labels.team,
    cpu: (.spec.containers[].resources.requests.cpu // "0m"),
    memory: (.spec.containers[].resources.requests.memory // "0Mi")
  }
' | jq -s '
  group_by(.team) |
  map({
    team: .[0].team,
    total_cpu: (map(.cpu | rtrimstr("m") | tonumber) | add),
    total_memory: (map(.memory | rtrimstr("Mi") | tonumber) | add)
  })
'

# Expected output:
# [
#   {"team": "team-a", "total_cpu": 1500, "total_memory": 3072},
#   {"team": "team-b", "total_cpu": 2000, "total_memory": 4096}
# ]
```

### Step 4: Cloud Provider Cost Allocation

**AWS EKS**: Use **Cost Allocation Tags**
```bash
# Tag namespace with AWS cost allocation tags
kubectl label namespace team-a-prod \
  "aws.amazon.com/cost-center=engineering" \
  "aws.amazon.com/team=team-a"

# AWS Cost Explorer will show costs by team/cost-center
```

**GCP GKE**: Use **GKE Cost Allocation**
```bash
# Enable GKE cost allocation (cluster creation)
gcloud container clusters create my-cluster \
  --enable-cost-allocation

# Label namespaces
kubectl label namespace team-a-prod \
  "gke-cost-allocation/team=team-a" \
  "gke-cost-allocation/cost-center=engineering"

# View costs in Cloud Billing console
```

**Azure AKS**: Use **Azure Tags**
```bash
# Tag resource group
az group update --name my-rg --tags team=team-a cost-center=engineering

# View costs in Cost Management
```

**✅ Success!** Finance knows exactly which team is driving cloud costs.

---

## 🤖 Part 6: Tenant Onboarding Automation (GitOps)

### Manual Onboarding (Tedious)

```bash
# For every new team, manually run:
kubectl create namespace team-d-prod
kubectl apply -f resourcequota.yaml
kubectl apply -f limitrange.yaml
kubectl apply -f networkpolicy.yaml
kubectl apply -f rbac.yaml
# ... 10 more YAMLs
```

**Problem**: Error-prone, inconsistent (Team A has quotas, Team D doesn't).

### Automated Onboarding (GitOps)

Create a **tenant template** in Git. New tenants are **just YAML commits**.

#### Step 1: Create Tenant Template

```yaml
# File: tenant-onboarding/template/tenant.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: {{ .TenantName }}-{{ .Environment }}
  labels:
    team: {{ .TenantName }}
    environment: {{ .Environment }}
    cost-center: {{ .CostCenter }}
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: {{ .TenantName }}-quota
  namespace: {{ .TenantName }}-{{ .Environment }}
spec:
  hard:
    requests.cpu: {{ .CpuQuota }}
    requests.memory: {{ .MemoryQuota }}
    count/pods: {{ .PodQuota }}
---
apiVersion: v1
kind: LimitRange
metadata:
  name: {{ .TenantName }}-limits
  namespace: {{ .TenantName }}-{{ .Environment }}
spec:
  limits:
  - type: Container
    default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: {{ .TenantName }}-isolation
  namespace: {{ .TenantName }}-{{ .Environment }}
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector: {}
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ .TenantName }}-admin
  namespace: {{ .TenantName }}-{{ .Environment }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: team-admin
  namespace: {{ .TenantName }}-{{ .Environment }}
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{ .TenantName }}-admin-binding
  namespace: {{ .TenantName }}-{{ .Environment }}
subjects:
- kind: ServiceAccount
  name: {{ .TenantName }}-admin
roleRef:
  kind: Role
  name: team-admin
  apiGroup: rbac.authorization.k8s.io
```

#### Step 2: Create Tenant Config

```yaml
# File: tenant-onboarding/tenants/team-d.yaml
tenantName: team-d
environment: prod
costCenter: marketing
cpuQuota: "6"
memoryQuota: 12Gi
podQuota: "80"
```

#### Step 3: Render Template (Helm or Kustomize)

**Using Helm**:
```bash
# Render tenant manifest
helm template team-d tenant-onboarding/template \
  -f tenant-onboarding/tenants/team-d.yaml \
  > manifests/team-d-prod.yaml

# Apply to cluster
kubectl apply -f manifests/team-d-prod.yaml
```

**Using ArgoCD** (GitOps):
```yaml
# File: argocd-apps/team-d.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: team-d-tenant
  namespace: argocd
spec:
  project: tenants
  source:
    repoURL: https://github.com/mycompany/tenant-onboarding
    path: template
    helm:
      valueFiles:
      - ../tenants/team-d.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: team-d-prod
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

**Result**: Commit `team-d.yaml` to Git → ArgoCD auto-creates namespace + quota + RBAC (zero manual work!).

**✅ Success!** New tenant onboarding takes **5 minutes** (just edit YAML + commit).

---

## 🎓 Key Takeaways

### What You Learned
1. **ResourceQuota**: Prevent one team from consuming entire cluster
2. **LimitRange**: Enforce default resource requests/limits
3. **NetworkPolicy**: Isolate tenants (no cross-namespace traffic)
4. **RBAC**: Team-specific permissions (admins vs. developers)
5. **Cost Allocation**: Label-based billing (who's spending what)
6. **GitOps Onboarding**: Automated tenant creation (template + config)

### Multi-Tenancy Maturity Model

| Level | Practices | Security | Cost |
|-------|-----------|----------|------|
| **Level 1: Basic** | Namespaces only | ❌ No isolation | 💰 Low |
| **Level 2: Intermediate** | + ResourceQuota + LimitRange | ⚠️ Resource isolation | 💰💰 Medium |
| **Level 3: Advanced** | + NetworkPolicy + RBAC | ✅ Full isolation | 💰💰 Medium |
| **Level 4: Production** | + Cost allocation + GitOps | ✅✅ Automated | 💰💰💰 High |

**This Lab**: Level 4 (production-ready).

### Production Checklist
- [ ] Enable ResourceQuota in all tenant namespaces
- [ ] Configure LimitRange (prevent unbounded pods)
- [ ] Apply NetworkPolicy (deny-all + whitelist)
- [ ] Create team-specific RBAC roles
- [ ] Label namespaces for cost allocation
- [ ] Automate tenant onboarding (Helm + ArgoCD)
- [ ] Monitor quota usage (Prometheus alerts when >80% used)
- [ ] Document tenant SLA (CPU, memory, storage limits)

---

## 🔄 What's Next?

- **[Lab 9.5: Complex Microservices](09.5-complex-microservices.md)** → Service mesh for multi-tenant apps
- **[Lab 11: GitOps with ArgoCD](11-gitops-argocd.md)** → Automate tenant onboarding
- **[Lab 6: Medical Security](06-medical-security.md)** → Advanced RBAC and security patterns

---

## 🤔 Common Questions

**Q: Soft vs. hard multi-tenancy?**  
A: **Soft**: Namespace isolation (NetworkPolicy + RBAC). Use for internal teams (trust assumed). **Hard**: Node isolation (taints + node pools). Use for external customers (zero trust).

**Q: Should we use node pools for each team?**  
A: Only for **hard multi-tenancy** (SaaS customers). For internal teams, soft multi-tenancy is cheaper (higher node utilization).

**Q: How do we handle shared services (databases, monitoring)?**  
A: Create a **shared-services namespace**. Use NetworkPolicy to allow tenant namespaces → shared-services (but not tenant ↔ tenant).

**Q: What about PriorityClasses (evict low-priority pods first)?**  
A: Great for multi-tenancy! Example:
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: production-high
value: 1000000  # Higher = less likely to be evicted
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: development-low
value: 1000
```
Then set `priorityClassName: production-high` in prod deployments.

**Q: How do we test NetworkPolicy without breaking production?**  
A: Use **audit mode** (some CNIs support it). Or test in staging namespace first. Always have a "break-glass" escape hatch (admin ClusterRole can bypass all policies).

**Q: What if a team needs MORE resources?**  
A: Update ResourceQuota via GitOps (commit YAML change → ArgoCD applies). Alternatively, allow teams to **request quota increases** via Slack bot → auto-updates Git repo.

---

**🎉 Congrats!** You can now run 100+ teams on one cluster like Shopify. Welcome to the multi-tenancy club! 🏢
