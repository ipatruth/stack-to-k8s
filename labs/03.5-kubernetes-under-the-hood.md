# Lab 3.5: Kubernetes Under the Hood
Understand how Kubernetes actually works: controller loops, etcd, API server flow, scheduler, and kubelet.

**Time**: 50 minutes  
**Difficulty**: ⭐⭐⭐ Advanced  
**Focus**: Kubernetes internals, Control plane, Controller patterns

---

## 🎯 Objective

Demystify the "magic" of Kubernetes. Learn how a simple `kubectl apply` triggers a cascade of events across the control plane, scheduler, and kubelet to create running pods.

Most Kubernetes courses teach **what** to do (deploy apps, create services). This lab teaches **how** Kubernetes does it internally.

---

## 📋 What You'll Learn

- **Controller loop pattern** (watch → compare → reconcile)
- **etcd**: The brain of Kubernetes (key-value store)
- **API server request flow** (authentication → authorization → admission → validation)
- **What happens during `kubectl apply`?** (trace YAML → running pod)
- **Scheduler decision-making** (node selection algorithm)
- **Kubelet pod lifecycle** (image pull → container runtime → health checks)
- **5 ways to break a deployment** (systematically break each component)

---

## 🧭 Prerequisites

**Required**: Completed Labs 1-3 (basic Kubernetes concepts)  
**Helpful**: Curiosity about "how does this actually work?"

**Tools needed**:
- `kubectl` (with cluster access)
- `etcdctl` (will install in this lab)
- `watch` command (Linux/macOS)

---

## 🏗️ Architecture: The Kubernetes Control Plane

Before diving in, let's visualize the control plane components:

```
┌─────────────────────────────────────────────────────────────────┐
│                     Kubernetes Control Plane                     │
│                                                                   │
│  ┌────────────┐  ┌──────────────┐  ┌──────────┐  ┌───────────┐ │
│  │ API Server │◄─┤ etcd (state) │  │Scheduler │  │Controller │ │
│  │  (kubectl) │  │  key-value   │  │(pod→node)│  │ Manager   │ │
│  │   talks    │  │    store     │  │ binding  │  │ (loops)   │ │
│  │    here    │  └──────────────┘  └──────────┘  └───────────┘ │
│  └──────┬─────┘                                                  │
│         │                                                        │
└─────────┼────────────────────────────────────────────────────────┘
          │
          │ (API calls)
          │
          ▼
┌─────────────────────────────────────────────────────────────────┐
│                        Worker Nodes                              │
│                                                                   │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │ Node 1: kubelet (watches API server for pod assignments) │   │
│  │   ├─ Container Runtime (containerd/Docker)               │   │
│  │   ├─ Pod: nginx-7d8b49557c-abc12                         │   │
│  │   └─ kube-proxy (iptables rules for Services)            │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                   │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │ Node 2: kubelet                                           │   │
│  │   ├─ Container Runtime                                    │   │
│  │   ├─ Pod: nginx-7d8b49557c-xyz89                         │   │
│  │   └─ kube-proxy                                           │   │
│  └──────────────────────────────────────────────────────────┘   │
└───────────────────────────────────────────────────────────────────┘
```

**Key Insight**: Kubernetes is a **distributed system** where components watch for changes and react independently. There's no "master orchestrator" - it's all controller loops!

---

## Phase 1: The Controller Loop Pattern (20 minutes)

### 📚 Theory: Watch → Compare → Reconcile

**Every Kubernetes controller** (Deployment, ReplicaSet, StatefulSet, etc.) follows the same pattern:

```
┌─────────────────────────────────────────────────────────┐
│              The Universal Controller Loop               │
│                                                          │
│   1. WATCH: Monitor API server for changes              │
│      (e.g., "Deployment spec changed")                  │
│              ▼                                           │
│   2. COMPARE: Desired state vs actual state             │
│      (e.g., "Want 3 replicas, have 2 replicas")         │
│              ▼                                           │
│   3. RECONCILE: Take action to match desired state      │
│      (e.g., "Create 1 more pod")                        │
│              ▼                                           │
│   4. REPEAT: Loop forever (every few seconds)           │
│              ▼                                           │
│   Back to step 1 ───────────────────────────────────────┘
```

**Example: Deployment Controller**

```
User runs: kubectl scale deployment nginx --replicas=5

┌─ Deployment Controller Loop ──────────────────────────┐
│ 1. WATCH: "Oh! Deployment 'nginx' spec changed"       │
│    Desired replicas: 5                                 │
│                                                        │
│ 2. COMPARE: Check current state                       │
│    kubectl get rs nginx-7d8b49557c                    │
│    Current replicas: 2                                 │
│    Status: 2 < 5 (need 3 more!)                       │
│                                                        │
│ 3. RECONCILE: Update ReplicaSet                       │
│    kubectl patch rs nginx-7d8b49557c -p '{"spec":     │
│      {"replicas":5}}'                                  │
│    → Triggers ReplicaSet controller!                   │
│                                                        │
│ 4. REPEAT: Watch for next change...                   │
└────────────────────────────────────────────────────────┘

┌─ ReplicaSet Controller Loop ──────────────────────────┐
│ 1. WATCH: "ReplicaSet 'nginx-7d8b49557c' changed!"    │
│    Desired replicas: 5                                 │
│                                                        │
│ 2. COMPARE: Count running pods                        │
│    kubectl get pods -l app=nginx                      │
│    Current pods: 2                                     │
│    Status: 2 < 5 (need 3 more!)                       │
│                                                        │
│ 3. RECONCILE: Create 3 new pods                       │
│    POST /api/v1/namespaces/default/pods               │
│    (3 times)                                           │
│    → Triggers Scheduler!                               │
│                                                        │
│ 4. REPEAT: Watch for next change...                   │
└────────────────────────────────────────────────────────┘
```

### 🔬 Hands-On: Watch the Deployment Controller in Action

Let's trigger the controller loop and watch it work:

```bash
# Create namespace
kubectl create namespace controller-demo

# Deploy nginx with 1 replica
kubectl create deployment nginx --image=nginx:1.25 --replicas=1 -n controller-demo

# In Terminal 1: Watch pods in real-time
watch -n 0.5 'kubectl get pods -n controller-demo -o wide'

# In Terminal 2: Watch ReplicaSet events
kubectl get rs -n controller-demo -w

# In Terminal 3: Scale up (trigger controller loop!)
kubectl scale deployment nginx --replicas=5 -n controller-demo
```

**What you'll see**:
1. **Deployment controller** updates ReplicaSet spec (desired: 5)
2. **ReplicaSet controller** creates 4 new pods (2 → 5)
3. **Scheduler** assigns each pod to a node
4. **Kubelet** (on each node) pulls image and starts containers

**Watch the timeline**:
```
T+0s:  User runs `kubectl scale`
T+1s:  Deployment controller updates ReplicaSet
T+2s:  ReplicaSet controller creates 4 pods (Pending state)
T+3s:  Scheduler assigns pods to nodes (pod.spec.nodeName set)
T+5s:  Kubelet pulls nginx:1.25 image (if not cached)
T+10s: Pods Running (all 5 replicas healthy)
```

### 🐛 Break It: What Happens If You Delete a Pod?

```bash
# Delete one pod manually
kubectl delete pod -n controller-demo $(kubectl get pods -n controller-demo -o name | head -1)

# Watch what happens in Terminal 1 (watch kubectl get pods)
# Expected: ReplicaSet controller creates a NEW pod immediately!
```

**Why?**
1. **ReplicaSet controller loop** detects: "I have 4 pods, but spec says 5!"
2. **Reconcile**: Create 1 new pod
3. **Result**: Self-healing! Kubernetes automatically recovers.

**This is the magic of controllers**: Declarative ("I want 5") vs Imperative ("Start pod A, start pod B...")

---

## Phase 2: etcd Deep Dive (15 minutes)

### 📚 Theory: etcd = The Brain of Kubernetes

**etcd is a distributed key-value store** that holds ALL Kubernetes state:
- Pod definitions
- Service configurations
- Secrets (base64-encoded)
- Node status
- ConfigMaps
- Everything!

**Critical insight**: When you `kubectl apply`, you're writing to etcd. When controllers reconcile, they read from etcd.

```
kubectl apply -f deployment.yaml
        ↓
API Server validates YAML
        ↓
Write to etcd: /registry/deployments/default/nginx
        ↓
Deployment controller watches etcd for changes
        ↓
Controller loop triggered!
```

### 🔬 Hands-On: Explore etcd Directly

**Warning**: Directly accessing etcd is **dangerous** in production! This is for learning only.

#### Step 1: Access etcd (Local Cluster Only!)

```bash
# For kind/minikube/k3d clusters, etcd runs in a pod
kubectl get pods -n kube-system | grep etcd

# Get etcd pod name
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o name)

# Install etcdctl inside the pod (Alpine Linux)
kubectl exec -n kube-system $ETCD_POD -- sh -c '
  apk add --no-cache etcd-ctl 2>/dev/null || true
'
```

#### Step 2: Read Kubernetes State from etcd

```bash
# List all keys in etcd (warning: LOTS of output!)
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  get / --prefix --keys-only | head -20

# Example output:
# /registry/deployments/default/nginx
# /registry/pods/default/nginx-7d8b49557c-abc12
# /registry/services/default/kubernetes
# /registry/secrets/default/my-secret
```

#### Step 3: Watch etcd Changes in Real-Time

```bash
# In Terminal 1: Watch etcd for pod changes
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  watch --prefix /registry/pods/controller-demo/

# In Terminal 2: Create a pod
kubectl run test-pod --image=nginx -n controller-demo

# Terminal 1 will show: PUT /registry/pods/controller-demo/test-pod
# (etcd records the pod creation!)
```

### 🎓 Key Takeaway: etcd is the Source of Truth

- **API server** is stateless (can restart without losing data)
- **etcd** is stateful (lose etcd = lose entire cluster state)
- **Backup etcd** = backup entire Kubernetes cluster
- **etcd failure** = total cluster failure (no reads/writes possible)

**Production tip**: Always run etcd with 3 or 5 replicas (HA with Raft consensus).

---

## Phase 3: kubectl apply Lifecycle (10 minutes)

### 📚 Theory: From YAML to Running Pod

What happens when you run `kubectl apply -f deployment.yaml`?

```
┌────────────────────────────────────────────────────────────────┐
│ Step 1: kubectl parses YAML and sends JSON to API server      │
│   POST /apis/apps/v1/namespaces/default/deployments           │
│   Body: {"apiVersion":"apps/v1","kind":"Deployment",...}       │
└────────────────────────────────────────────────────────────────┘
                             ▼
┌────────────────────────────────────────────────────────────────┐
│ Step 2: API Server validates request                          │
│   ✅ Authentication: Is this user valid? (check kubeconfig)   │
│   ✅ Authorization: Can this user create deployments? (RBAC)  │
│   ✅ Admission: Run webhooks (e.g., Pod Security Standards)   │
│   ✅ Validation: Is YAML schema valid? (missing fields?)      │
└────────────────────────────────────────────────────────────────┘
                             ▼
┌────────────────────────────────────────────────────────────────┐
│ Step 3: API Server writes to etcd                             │
│   etcdctl put /registry/deployments/default/nginx <JSON>      │
│   Returns: HTTP 201 Created                                    │
└────────────────────────────────────────────────────────────────┘
                             ▼
┌────────────────────────────────────────────────────────────────┐
│ Step 4: Deployment Controller detects change (watch event)    │
│   Controller loop: "New deployment! Create ReplicaSet!"       │
│   POST /apis/apps/v1/namespaces/default/replicasets           │
└────────────────────────────────────────────────────────────────┘
                             ▼
┌────────────────────────────────────────────────────────────────┐
│ Step 5: ReplicaSet Controller detects change                  │
│   Controller loop: "Need 3 pods! Create them!"                │
│   POST /api/v1/namespaces/default/pods (3 times)              │
└────────────────────────────────────────────────────────────────┘
                             ▼
┌────────────────────────────────────────────────────────────────┐
│ Step 6: Scheduler detects unscheduled pods                    │
│   For each pod: Find best node (CPU, memory, affinity rules)  │
│   PATCH /api/v1/namespaces/default/pods/nginx-xxx             │
│   Set: spec.nodeName = "worker-node-2"                        │
└────────────────────────────────────────────────────────────────┘
                             ▼
┌────────────────────────────────────────────────────────────────┐
│ Step 7: Kubelet (on worker-node-2) detects pod assignment     │
│   Kubelet watches: "Pod nginx-xxx assigned to me!"            │
│   1. Pull image: docker pull nginx:1.25                       │
│   2. Create container: docker run nginx:1.25                  │
│   3. Start health checks: HTTP GET /healthz                   │
│   4. Report status to API server: Pod Running                 │
└────────────────────────────────────────────────────────────────┘
                             ▼
                    Pod Running! ✅
```

### 🔬 Hands-On: Trace the Lifecycle

```bash
# In Terminal 1: Watch API server logs (if accessible)
# kubectl logs -n kube-system kube-apiserver-xxx --follow

# In Terminal 2: Watch events in real-time
kubectl get events -n controller-demo --watch

# In Terminal 3: Deploy nginx
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lifecycle-demo
  namespace: controller-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: lifecycle-demo
  template:
    metadata:
      labels:
        app: lifecycle-demo
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 80
EOF
```

**Watch Terminal 2** (events) to see the cascade:
```
0s    Normal  ScalingReplicaSet  deployment/lifecycle-demo  Scaled up replica set lifecycle-demo-7d8b49557c to 2
0s    Normal  SuccessfulCreate   replicaset/lifecycle-demo  Created pod: lifecycle-demo-7d8b49557c-abc12
0s    Normal  SuccessfulCreate   replicaset/lifecycle-demo  Created pod: lifecycle-demo-7d8b49557c-xyz89
1s    Normal  Scheduled          pod/lifecycle-demo-abc12   Successfully assigned to node worker-1
1s    Normal  Scheduled          pod/lifecycle-demo-xyz89   Successfully assigned to node worker-2
3s    Normal  Pulling            pod/lifecycle-demo-abc12   Pulling image "nginx:1.25"
5s    Normal  Pulled             pod/lifecycle-demo-abc12   Successfully pulled image
6s    Normal  Created            pod/lifecycle-demo-abc12   Created container nginx
6s    Normal  Started            pod/lifecycle-demo-abc12   Started container nginx
```

**Each event = a different controller taking action!**

---

## Phase 4: The Scheduler's Decision-Making (5 minutes)

### 📚 Theory: How Does Scheduler Choose a Node?

When a pod is created without `spec.nodeName`, the **Scheduler** must pick a node:

```
┌─ Scheduler Algorithm ─────────────────────────────────────┐
│                                                            │
│ 1. FILTER: Eliminate unsuitable nodes                     │
│    ❌ Node has insufficient CPU/memory                    │
│    ❌ Node has no matching labels (nodeSelector)          │
│    ❌ Node is cordoned (kubectl cordon)                   │
│    ❌ Pod has anti-affinity with existing pods on node    │
│                                                            │
│ 2. SCORE: Rank remaining nodes (0-100 points)             │
│    📊 Spread pods evenly (avoid hotspots)                 │
│    📊 Prefer nodes with image already cached              │
│    📊 Respect pod affinity/anti-affinity preferences      │
│    📊 Balance resource usage (CPU/memory)                 │
│                                                            │
│ 3. BIND: Choose highest-scoring node                      │
│    PATCH /api/v1/namespaces/default/pods/nginx-xxx        │
│    Set: spec.nodeName = "worker-node-3"                   │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

### 🔬 Hands-On: Watch Scheduler Work

```bash
# Create a pod with high CPU request (scheduler must find node with capacity)
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: high-cpu-pod
  namespace: controller-demo
spec:
  containers:
  - name: stress
    image: polinux/stress
    command: ["stress"]
    args: ["--cpu", "2"]
    resources:
      requests:
        cpu: "2000m"  # 2 full CPU cores
        memory: "1Gi"
EOF

# Watch scheduler events
kubectl get events -n controller-demo --field-selector involvedObject.name=high-cpu-pod

# Expected event:
# Normal  Scheduled  pod/high-cpu-pod  Successfully assigned to node worker-3
#   Reason: worker-3 has 4 CPUs available, workers 1-2 only have 1 CPU free
```

---

## Phase 5: 5 Ways to Break a Deployment (Debugging Practice!)

Now that you understand the internals, let's **intentionally break things** to learn how failures manifest:

### 🐛 Break #1: Overload etcd (Simulate etcd Failure)

```bash
# This simulates what happens when etcd is unreachable

# Stop etcd (DON'T DO THIS IN PRODUCTION!)
kubectl delete pod -n kube-system $(kubectl get pods -n kube-system -l component=etcd -o name)

# Try to create a pod
kubectl run test --image=nginx -n controller-demo

# Expected result: Hangs forever!
# Why? API server can't write to etcd, so request never completes.

# Symptoms in real clusters:
# - kubectl commands timeout
# - Controllers stop reconciling (can't read state)
# - Cluster appears "frozen"

# Fix: etcd pod restarts automatically (StatefulSet with restart policy)
# Wait 30 seconds, then retry kubectl command
```

### 🐛 Break #2: Crash the Scheduler

```bash
# Delete scheduler pod
kubectl delete pod -n kube-system $(kubectl get pods -n kube-system -l component=kube-scheduler -o name)

# Create a pod
kubectl run unscheduled-pod --image=nginx -n controller-demo

# Check pod status
kubectl get pods -n controller-demo unscheduled-pod

# Expected: STATUS = Pending forever!
# Why? No scheduler to assign nodeName.

# Check events
kubectl describe pod unscheduled-pod -n controller-demo
# Expected event: "0/3 nodes are available: waiting for scheduler"

# Fix: Scheduler pod restarts automatically
# Once restarted, pod will be scheduled within seconds
```

### 🐛 Break #3: Kill Controller Manager (Controllers Stop Reconciling)

```bash
# Delete controller-manager pod
kubectl delete pod -n kube-system $(kubectl get pods -n kube-system -l component=kube-controller-manager -o name)

# Scale a deployment
kubectl scale deployment nginx --replicas=10 -n controller-demo

# Watch pods
kubectl get pods -n controller-demo -w

# Expected: Pods DON'T scale up! (stuck at old replica count)
# Why? Deployment controller isn't running, so no reconciliation.

# Fix: Controller-manager pod restarts automatically
# Once restarted, scaling resumes immediately
```

### 🐛 Break #4: Disable Kubelet on a Node (Pods Can't Start)

```bash
# This simulates node failure

# Cordon a node (prevent new pods from scheduling there)
NODE=$(kubectl get nodes -o name | head -1)
kubectl cordon $NODE

# Try to schedule a pod
kubectl run node-test --image=nginx -n controller-demo

# If all nodes are cordoned, pod stays Pending
# Events: "0/3 nodes are available: 3 node(s) were unschedulable"

# Uncordon the node
kubectl uncordon $NODE

# Pod should schedule within seconds
```

### 🐛 Break #5: Invalid YAML (API Server Rejects It)

```bash
# Try to create a deployment with invalid YAML
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: broken-deployment
  namespace: controller-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: WRONG-LABEL  # ❌ Selector doesn't match template labels!
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
EOF

# Expected error:
# Error: deployment.apps "broken-deployment" is invalid:
#   spec.template.metadata.labels: Invalid value: map[string]string{"app":"WRONG-LABEL"}:
#   `selector` does not match template `labels`

# This is caught by API server validation (before etcd write)
```

---

## 💡 Key Takeaways

### 1. **Controller Loop Pattern is Universal**
Every Kubernetes resource (Deployment, StatefulSet, Service) has a controller that:
- **Watches** for changes
- **Compares** desired vs actual state
- **Reconciles** by taking action
- **Repeats** forever

### 2. **etcd is the Source of Truth**
- API server is stateless (requests go through it)
- etcd stores ALL cluster state
- Controllers read from etcd to make decisions
- **Backup etcd = backup entire cluster**

### 3. **kubectl apply is a Multi-Step Process**
```
YAML → kubectl → API Server → etcd → Controllers → Scheduler → Kubelet → Running Pod
```
Each step can fail independently!

### 4. **Failures Manifest Differently**
- **etcd down**: All kubectl commands hang
- **Scheduler down**: Pods stuck in Pending (no nodeName assigned)
- **Controller-manager down**: Deployments don't scale, self-healing stops
- **Kubelet down**: Pods on that node become NotReady
- **API server down**: Nothing works (no communication possible)

### 5. **Self-Healing is Automatic**
- Delete a pod → ReplicaSet controller creates new one
- Node fails → Pods rescheduled to healthy nodes
- Control plane pod crashes → Kubernetes restarts it (static pods)

---

## 🔨 Challenge: Build Your Own Controller (Optional)

Want to truly understand controllers? **Write your own!**

Here's a simple controller that watches for pods with label `auto-delete: true` and deletes them after 60 seconds:

```python
# simple-controller.py
import time
from kubernetes import client, config, watch

config.load_kube_config()
v1 = client.CoreV1Api()

print("🚀 Starting simple controller...")
print("Watching for pods with label 'auto-delete: true'")

w = watch.Watch()
created_pods = {}  # Track pod creation time

# Controller loop!
for event in w.stream(v1.list_namespaced_pod, namespace="controller-demo"):
    pod = event['object']
    event_type = event['type']
    
    # Check if pod has auto-delete label
    if pod.metadata.labels and pod.metadata.labels.get('auto-delete') == 'true':
        pod_name = pod.metadata.name
        
        if event_type == 'ADDED':
            # WATCH: New pod detected!
            created_pods[pod_name] = time.time()
            print(f"⏱️  Pod {pod_name} created, will delete in 60s")
        
        # COMPARE: Has 60 seconds passed?
        if pod_name in created_pods:
            elapsed = time.time() - created_pods[pod_name]
            if elapsed > 60:
                # RECONCILE: Delete the pod!
                print(f"💣 Deleting pod {pod_name} (lived for {elapsed:.0f}s)")
                v1.delete_namespaced_pod(pod_name, "controller-demo")
                del created_pods[pod_name]
```

**Test it**:
```bash
# Run controller
python3 simple-controller.py

# In another terminal, create a pod with auto-delete label
kubectl run auto-delete-test --image=nginx --labels=auto-delete=true -n controller-demo

# Watch: Pod will be deleted after 60 seconds!
```

**This is how Kubernetes works!** Every controller is just a loop watching for changes and taking action.

---

## 🧹 Cleanup

```bash
# Delete namespace
kubectl delete namespace controller-demo

# This triggers a cascade:
# 1. Namespace controller deletes all resources in namespace
# 2. Deployment controllers stop managing deployments
# 3. ReplicaSet controllers delete pods
# 4. Kubelet stops containers
# 5. etcd removes all keys under /registry/.../controller-demo/
```

---


---

## 🎖️ Expert Mode: Control Plane Performance Tuning

> 💡 **Optional Challenge** — Ready to tune production clusters? **This is NOT required** to progress, but completing it unlocks the **⚙️ Control Plane Architect** badge!

**⏱️ Time**: +25 minutes  
**🎯 Difficulty**: ⭐⭐⭐⭐⭐ (Expert)  
**📋 Prerequisites**: Complete Lab 3.5 etcd Deep Dive section above

### The Scenario

Your API server is timing out. `kubectl get pods` takes 5+ seconds. Prometheus shows high etcd disk latency:

```bash
# etcd_disk_wal_fsync_duration_seconds: 45ms (should be <10ms)
# etcd_mvcc_db_total_size_in_bytes: 8.5GB (massive!)
```

**Your cluster is slow because etcd is overwhelmed.**

### Challenge: Tune etcd for Production Performance

**Your Mission**:
1. Check etcd metrics (disk latency, DB size)
2. Compact old revisions
3. Defragment to reclaim space
4. Configure auto-compaction
5. Optimize disk I/O

**Hints**:
```bash
# Port-forward to etcd
kubectl port-forward -n kube-system etcd-<node> 2379:2379

# Check metrics
curl http://localhost:2379/metrics | grep etcd_disk

# Check DB size
ETCDCTL_API=3 etcdctl endpoint status --write-out=table

# Compact (keeps last 1000 revisions)
ETCDCTL_API=3 etcdctl compact <revision-1000>

# Defragment
ETCDCTL_API=3 etcdctl defrag --cluster
```

### Expected Outcome

- ✅ etcd disk latency < 10ms
- ✅ Database size reduced by 50%+
- ✅ API server response time < 500ms
- ✅ Auto-compaction configured

### Deep Dive: What You're Learning

**Production Skills**:
- etcd performance diagnostics
- Compaction strategies
- Disk I/O optimization
- Control plane tuning

**Interview Topics**:
- "etcd is slow, how do you diagnose and fix it?"
- "What's the difference between compaction and defragmentation?"
- "How do you monitor control plane health?"

**Real-World Impact**: **Reddit's 2023 outage** was caused by etcd running out of space due to no auto-compaction. Database grew to 12GB, API server became unresponsive. Engineers who knew this saved the platform.

### Complete Guide

For detailed etcd tuning procedures, see:  
**[Senior K8s Debugging Guide: etcd Performance](../../docs/reference/senior-k8s-debugging.md#21-etcd-performance-bottlenecks)**

### Badge Unlocked! 🎉

Complete this challenge and you've earned:  
**⚙️ Control Plane Architect** — You can tune production Kubernetes clusters!

**Track your progress**: [Lab Progress Tracker](../../docs/learning/LAB-PROGRESS.md#expert-badges)

## 🎓 What's Next?

Now that you understand Kubernetes internals, you can:
- **Debug faster**: Know which component is failing (scheduler vs kubelet vs controller)
- **Optimize performance**: Tune controller sync intervals, etcd performance
- **Build custom controllers**: Extend Kubernetes with your own logic (Operators!)
- **Appreciate the design**: Declarative, self-healing, resilient by default

**Recommended Next Labs**:
- **Lab 4**: Kubernetes Fundamentals (now you know the "why" behind the commands!)
- **Lab 9**: Helm Package Management (Helm generates YAML that goes through this same flow)
- **Lab 10**: GitOps with ArgoCD (ArgoCD is a controller watching Git repos!)

---

## 📚 Further Reading

- **Kubernetes Design Patterns**: https://kubernetes.io/docs/concepts/architecture/
- **Controller Runtime**: https://github.com/kubernetes-sigs/controller-runtime
- **Writing Controllers**: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/
- **etcd Documentation**: https://etcd.io/docs/

**You now understand Kubernetes at a level most engineers never reach. Use this knowledge wisely!** 🚀
