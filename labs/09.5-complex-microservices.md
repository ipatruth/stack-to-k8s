# Lab 9.5: Complex Microservices Architecture 🕸️

**⏱ Time**: 90 minutes  
**🎯 Difficulty**: ⭐⭐⭐⭐⭐ Master  
**📋 Prerequisites**: Labs 7 (Monitoring), 9 (Helm), 11 (GitOps)

---

## 🎯 Objective

**The Problem**: You've deployed simple apps. Real production systems have **10+ microservices** talking to each other:
- Frontend → API Gateway → Auth Service → User Service → Database
- Payment Service → Notification Service → Email Queue → SMTP Service

**Without proper tools**:
- 🔥 Debugging is impossible (which service failed?)
- 🔥 Cascading failures (one slow service kills everything)
- 🔥 No traffic control (can't canary deploy safely)
- 🔥 Security holes (services talk in plaintext)

**This Lab**: Deploy a **service mesh** (Linkerd) with:
1. **Distributed tracing** (find slow requests across 5+ services)
2. **Traffic splitting** (canary deployments with 10% traffic)
3. **Circuit breakers** (auto-isolate failing services)
4. **mTLS** (encrypted service-to-service communication)
5. **Service profiles** (per-route metrics and retries)

**Why This Matters**: Netflix runs **700+ microservices**. This lab teaches the patterns that make it possible.

---

## 📚 Key Concepts

### What is a Service Mesh?

**Without Service Mesh** (traditional):
```
Frontend → Backend
   ↓
HTTP request (plaintext)
No retries, no timeouts, no tracing
If backend fails, frontend crashes
```

**With Service Mesh**:
```
Frontend + Sidecar Proxy → Backend + Sidecar Proxy
                ↓
        mTLS encrypted
        Auto-retries (3x)
        Circuit breaker (fail fast)
        Distributed tracing (Jaeger)
```

**Service Mesh Options**:

| Mesh | Complexity | Performance | Features | Adoption |
|------|------------|-------------|----------|----------|
| **Linkerd** | ⭐⭐ Simple | 🚀 Fast (Rust proxy) | Basic (enough for 90%) | Growing |
| **Istio** | ⭐⭐⭐⭐⭐ Complex | 🐢 Slower (Envoy) | Advanced (everything) | Most popular |
| **Consul Connect** | ⭐⭐⭐ Medium | 🚀 Fast | HashiCorp ecosystem | Medium |

**This Lab**: Linkerd (easiest to learn, production-ready).

---

## 🔧 Setup: Install Linkerd

### Step 1: Install Linkerd CLI

```bash
# macOS
brew install linkerd

# Linux
curl -fsL https://run.linkerd.io/install | sh
export PATH=$PATH:$HOME/.linkerd2/bin

# Windows (PowerShell as Administrator)
# Download from: https://github.com/linkerd/linkerd2/releases
# Extract and add to PATH, or use Scoop:
scoop bucket add linkerd https://github.com/linkerd/scoop-linkerd
scoop install linkerd

# Verify
linkerd version
# Expected: Client version: stable-2.14.x
```

### Step 2: Pre-Flight Check

```bash
# Check if cluster is ready for Linkerd
linkerd check --pre
# Expected: All checks pass ✔

# If failures, common fixes:
# - Missing: kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)
```

### Step 3: Install Linkerd Control Plane

```bash
# Install Linkerd CRDs
linkerd install --crds | kubectl apply -f -

# Install Linkerd control plane
linkerd install | kubectl apply -f -

# Wait for control plane (2-3 minutes)
linkerd check
# Expected: All checks pass ✔

# Verify control plane pods
kubectl get pods -n linkerd
# Expected: linkerd-destination, linkerd-identity, linkerd-proxy-injector (all Running)
```

### Step 4: Install Linkerd Viz (Dashboard + Metrics)

```bash
# Install visualization components
linkerd viz install | kubectl apply -f -

# Wait for viz pods
kubectl rollout status -n linkerd-viz deploy

# Open Linkerd dashboard
linkerd viz dashboard &
# Opens http://localhost:50750 (Grafana + service topology)
```

**✅ Linkerd installed!** Now let's deploy a microservices app.

---

## 🏗️ Part 1: Deploy Multi-Service Application

### Architecture

```
┌─────────────┐
│   Frontend  │ (React app)
└──────┬──────┘
       │
       ↓ HTTP
┌─────────────┐
│ API Gateway │ (aggregates data)
└──────┬──────┘
       │
       ├─────────────┬─────────────┬─────────────┐
       ↓             ↓             ↓             ↓
┌───────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐
│   Auth    │ │   User   │ │ Product  │ │ Payment  │
│  Service  │ │ Service  │ │ Service  │ │ Service  │
└─────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘
      │            │            │            │
      ↓            ↓            ↓            ↓
   Redis      PostgreSQL   PostgreSQL      Stripe
```

### Step 1: Create Namespace and Enable Mesh

```bash
# Create namespace
kubectl create namespace microservices

# Enable automatic sidecar injection (magic!)
kubectl annotate namespace microservices linkerd.io/inject=enabled

# Verify annotation
kubectl get namespace microservices -o yaml | grep linkerd
# Expected: linkerd.io/inject: enabled
```

### Step 2: Deploy Backend Services

```yaml
kubectl apply -f - <<EOF
---
# Auth Service (validates JWT tokens)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
  namespace: microservices
spec:
  replicas: 2
  selector:
    matchLabels:
      app: auth-service
  template:
    metadata:
      labels:
        app: auth-service
        version: v1
    spec:
      containers:
      - name: auth
        image: buoyantio/bb-auth:v0.0.6  # Demo service
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
---
apiVersion: v1
kind: Service
metadata:
  name: auth-service
  namespace: microservices
spec:
  selector:
    app: auth-service
  ports:
  - port: 8080
    targetPort: 8080
---
# User Service (manages user profiles)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: microservices
spec:
  replicas: 2
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
        version: v1
    spec:
      containers:
      - name: user
        image: buoyantio/bb-user:v0.0.6
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
        - name: AUTH_SERVICE_URL
          value: "http://auth-service:8080"
---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: microservices
spec:
  selector:
    app: user-service
  ports:
  - port: 8080
    targetPort: 8080
---
# Product Service (catalog data)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-service
  namespace: microservices
spec:
  replicas: 3
  selector:
    matchLabels:
      app: product-service
  template:
    metadata:
      labels:
        app: product-service
        version: v1
    spec:
      containers:
      - name: product
        image: buoyantio/bb-product:v0.0.6
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
---
apiVersion: v1
kind: Service
metadata:
  name: product-service
  namespace: microservices
spec:
  selector:
    app: product-service
  ports:
  - port: 8080
    targetPort: 8080
---
# Payment Service (processes transactions)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-service
  namespace: microservices
spec:
  replicas: 2
  selector:
    matchLabels:
      app: payment-service
  template:
    metadata:
      labels:
        app: payment-service
        version: v1
    spec:
      containers:
      - name: payment
        image: buoyantio/bb-payment:v0.0.6
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
---
apiVersion: v1
kind: Service
metadata:
  name: payment-service
  namespace: microservices
spec:
  selector:
    app: payment-service
  ports:
  - port: 8080
    targetPort: 8080
EOF

# Wait for pods (watch sidecar injection!)
kubectl get pods -n microservices
# Expected: Each pod has 2 containers (app + linkerd-proxy)
```

### Step 3: Deploy API Gateway

```yaml
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  namespace: microservices
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
        version: v1
    spec:
      containers:
      - name: gateway
        image: buoyantio/bb-gateway:v0.0.6
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
        - name: AUTH_SERVICE_URL
          value: "http://auth-service:8080"
        - name: USER_SERVICE_URL
          value: "http://user-service:8080"
        - name: PRODUCT_SERVICE_URL
          value: "http://product-service:8080"
        - name: PAYMENT_SERVICE_URL
          value: "http://payment-service:8080"
---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway
  namespace: microservices
spec:
  selector:
    app: api-gateway
  ports:
  - port: 8080
    targetPort: 8080
  type: LoadBalancer  # Expose externally
EOF
```

### Step 4: Verify Mesh Injection

```bash
# Check that pods have Linkerd sidecar
kubectl get pods -n microservices -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].name}{"\n"}{end}'
# Expected: Each pod has: <app-name> + linkerd-proxy

# Check mesh status
linkerd -n microservices check --proxy
# Expected: All checks pass ✔

# View meshed services
linkerd -n microservices viz stat deploy
# Expected: Success rate, RPS, latency for each service
```

**✅ Microservices deployed!** All traffic is now **mTLS encrypted** and **observable**.

---

## 🔍 Part 2: Distributed Tracing (Find Slow Requests)

### Step 1: Generate Traffic

```bash
# Get LoadBalancer IP
GATEWAY_IP=$(kubectl get svc api-gateway -n microservices -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Send requests
for i in {1..100}; do
  curl -s http://$GATEWAY_IP:8080/api/users | jq .
  sleep 0.5
done
```

### Step 2: View Service Topology

```bash
# Open Linkerd dashboard
linkerd viz dashboard

# Navigate to: microservices namespace → Deployments
# Click on "api-gateway" → Tap (live request viewer)
```

**What You See**:
- Real-time traffic flow (API Gateway → Auth → User)
- Success rate per service (99.5%)
- Latency per service (Auth: 20ms, User: 50ms)
- Request volume (RPS)

### Step 3: Enable Distributed Tracing (Jaeger)

```bash
# Install Jaeger
linkerd jaeger install | kubectl apply -f -

# Wait for Jaeger
kubectl rollout status -n linkerd-jaeger deploy

# Verify
kubectl get pods -n linkerd-jaeger
# Expected: jaeger pod Running
```

### Step 4: View Traces

```bash
# Open Jaeger UI
linkerd jaeger dashboard &
# Opens http://localhost:16686

# In Jaeger UI:
# - Service: api-gateway
# - Click "Find Traces"
# - Select a trace (shows full request flow)
```

**Example Trace**:
```
api-gateway (100ms total)
├── auth-service (20ms)
└── user-service (50ms)
    └── PostgreSQL query (30ms)  ← Bottleneck found!
```

**✅ Success!** You can now debug slow requests across multiple services.

---

## 🚦 Part 3: Traffic Splitting (Canary Deployments)

### Step 1: Deploy v2 of User Service (with Bug)

```yaml
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v2
  namespace: microservices
spec:
  replicas: 2
  selector:
    matchLabels:
      app: user-service
      version: v2
  template:
    metadata:
      labels:
        app: user-service
        version: v2
    spec:
      containers:
      - name: user
        image: buoyantio/bb-user:v0.0.6
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
        - name: FAILURE_RATE  # Simulate 50% errors
          value: "0.5"
EOF

# Now we have:
# - user-service (v1): 2 replicas, 0% errors
# - user-service-v2 (v2): 2 replicas, 50% errors
```

### Step 2: Create TrafficSplit (10% to v2)

```yaml
kubectl apply -f - <<EOF
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  name: user-service-split
  namespace: microservices
spec:
  service: user-service  # Root service
  backends:
  - service: user-service
    weight: 900  # 90% traffic to v1
  - service: user-service-v2
    weight: 100  # 10% traffic to v2 (canary)
EOF
```

### Step 3: Monitor Canary Success Rate

```bash
# View success rate by version
linkerd -n microservices viz stat deploy --to deploy/user-service -t 1m
linkerd -n microservices viz stat deploy --to deploy/user-service-v2 -t 1m

# Expected:
# user-service (v1): 100% success rate
# user-service-v2 (v2): 50% success rate ❌ (bug detected!)
```

### Step 4: Rollback (0% to v2)

```bash
# Update TrafficSplit to 0% v2
kubectl patch trafficsplit user-service-split -n microservices --type json -p='[
  {"op": "replace", "path": "/spec/backends/0/weight", "value": 1000},
  {"op": "replace", "path": "/spec/backends/1/weight", "value": 0}
]'

# Verify all traffic goes to v1
linkerd -n microservices viz stat deploy --to deploy/user-service -t 1m
# Expected: 100% success rate (v2 receives no traffic)

# Delete buggy v2
kubectl delete deploy user-service-v2 -n microservices
```

**✅ Success!** Caught bug in canary (only 10% of users affected, not 100%).

---

## ⚡ Part 4: Circuit Breaker (Fail Fast)

### Step 1: Deploy Flaky Service

```yaml
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flaky-service
  namespace: microservices
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flaky-service
  template:
    metadata:
      labels:
        app: flaky-service
    spec:
      containers:
      - name: flaky
        image: buoyantio/bb-flaky:v0.0.6
        ports:
        - containerPort: 8080
        env:
        - name: BACKEND_PORT
          value: "8080"
        - name: FAILURE_RATE
          value: "0.9"  # 90% requests fail
---
apiVersion: v1
kind: Service
metadata:
  name: flaky-service
  namespace: microservices
spec:
  selector:
    app: flaky-service
  ports:
  - port: 8080
    targetPort: 8080
EOF
```

### Step 2: Create ServiceProfile (Circuit Breaker)

```yaml
kubectl apply -f - <<EOF
apiVersion: linkerd.io/v1alpha2
kind: ServiceProfile
metadata:
  name: flaky-service.microservices.svc.cluster.local
  namespace: microservices
spec:
  routes:
  - name: GET /api/data
    condition:
      method: GET
      pathRegex: /api/data
    responseClasses:
    - condition:
        status:
          min: 500
          max: 599
      isFailure: true  # 5xx = failure
    retries:
      limit: 3  # Retry up to 3 times
      timeout: 500ms  # Each retry timeout
    timeout: 2s  # Total request timeout
EOF
```

### Step 3: Test Circuit Breaker

```bash
# Send requests to flaky service
for i in {1..20}; do
  kubectl exec -n microservices deploy/api-gateway -- curl -s http://flaky-service:8080/api/data
done

# Check stats
linkerd -n microservices viz stat svc/flaky-service --to deploy/flaky-service
# Expected: 
# - Success rate: ~10% (without retries would be 10%)
# - Latency: Lower (fail fast instead of hanging)

# View retries in Linkerd dashboard
linkerd viz dashboard
# Navigate to flaky-service → Routes → See "Actual success rate" vs "Effective success rate"
```

**What Happened**:
1. First request fails (500 error)
2. Linkerd **auto-retries** (3x)
3. After 3 failures, **circuit opens** (fast-fail for 30s)
4. Some requests succeed on retries (effective success rate > actual)

**✅ Success!** Circuit breaker prevents cascading failures (API Gateway doesn't hang).

---

## 🔐 Part 5: mTLS (Encrypted Service-to-Service)

### Verify mTLS is Enabled

```bash
# Check mTLS status
linkerd -n microservices viz edges deployment
# Expected: All connections show "secured" with identity

# View certificate details
linkerd -n microservices viz tap deploy/api-gateway -o wide
# Expected: TLS column shows "true"
```

### What's Happening?

1. **Automatic Certificate Issuance**: Linkerd control plane issues X.509 certs to each pod (24-hour TTL)
2. **Transparent Encryption**: Linkerd proxy encrypts all pod-to-pod traffic (no app code changes!)
3. **Identity Verification**: Each pod proves identity via cert (prevents impersonation)

### Test mTLS Enforcement

```bash
# Try to access service from pod WITHOUT Linkerd proxy (should work but unencrypted)
kubectl run curl-test --image=curlimages/curl -n microservices --rm -it -- sh
# Inside pod:
curl http://user-service:8080/api/users
# ✅ Works (Linkerd allows unencrypted fallback by default)

# Enable strict mTLS (reject unencrypted)
kubectl annotate serviceprofile flaky-service.microservices.svc.cluster.local \
  -n microservices \
  config.linkerd.io/default-inbound-policy=all-authenticated

# Try again from unmeshed pod (should fail)
kubectl run curl-test --image=curlimages/curl -n microservices --rm -it -- sh
curl http://flaky-service:8080/api/data
# ❌ Expected: Connection refused (no valid cert)
```

**✅ Success!** All service-to-service traffic is encrypted (zero app changes).

---

## 📊 Part 6: Service Profiles (Per-Route Metrics)

### Step 1: Auto-Generate ServiceProfile

```bash
# Generate ServiceProfile from live traffic
linkerd -n microservices viz profile --tap deploy/api-gateway --tap-duration 30s api-gateway > api-gateway-profile.yaml

# Apply profile
kubectl apply -f api-gateway-profile.yaml
```

### Step 2: View Per-Route Metrics

```bash
# View metrics per route
linkerd -n microservices viz routes deploy/api-gateway
# Expected:
# ROUTE                    SUCCESS  RPS    LATENCY_P50  LATENCY_P99
# GET /api/users           100%     10.5   20ms         45ms
# GET /api/products        99.8%    15.2   30ms         80ms
# POST /api/checkout       98.5%    2.1    200ms        500ms ← Slow!
```

### Step 3: Configure Per-Route Retries

```yaml
kubectl apply -f - <<EOF
apiVersion: linkerd.io/v1alpha2
kind: ServiceProfile
metadata:
  name: payment-service.microservices.svc.cluster.local
  namespace: microservices
spec:
  routes:
  - name: POST /api/charge
    condition:
      method: POST
      pathRegex: /api/charge
    retries:
      limit: 5  # Retry payments 5x (idempotent)
      timeout: 2s
  - name: GET /api/balance
    condition:
      method: GET
      pathRegex: /api/balance
    retries:
      limit: 3  # Retry balance checks 3x
      timeout: 500ms
EOF
```

**✅ Success!** Different retry policies per route (GET vs POST).

---

## 🎓 Key Takeaways

### What You Learned
1. **Service Mesh**: Linkerd injects sidecar proxies (auto mTLS + retries + metrics)
2. **Distributed Tracing**: Jaeger shows request flow across services (find bottlenecks)
3. **Traffic Splitting**: Canary deployments (10% → 50% → 100%)
4. **Circuit Breaker**: Auto-retries + fail-fast (prevent cascading failures)
5. **mTLS**: All service-to-service traffic encrypted (zero app changes)
6. **Service Profiles**: Per-route metrics, retries, timeouts

### Service Mesh vs. Traditional

| Without Service Mesh | With Service Mesh (Linkerd) |
|----------------------|------------------------------|
| Plaintext traffic | mTLS encrypted (automatic) |
| No retries | Auto-retry (configurable) |
| No circuit breaker | Fail-fast after 3 failures |
| No distributed tracing | Jaeger traces across services |
| Manual canary deploys | TrafficSplit (10% → 100%) |
| Per-service metrics only | Per-route metrics (GET vs POST) |

### Production Checklist
- [ ] Enable Linkerd in production namespace
- [ ] Configure ServiceProfiles for all services
- [ ] Enable Jaeger for distributed tracing
- [ ] Set up TrafficSplit for canary deploys
- [ ] Configure circuit breakers (retries + timeouts)
- [ ] Enable strict mTLS (all-authenticated policy)
- [ ] Monitor "effective success rate" (Linkerd dashboard)
- [ ] Alert on low success rates (<99%)

---

## 🔄 What's Next?

- **[Lab 13: AI/ML GPU Workloads](13-ai-ml-gpu.md)** → Deploy ML models on K8s *(coming soon)*
- **[Lab 11: GitOps with ArgoCD](11-gitops-argocd.md)** → Automate service mesh config
- **[Lab 8.5: Multi-Tenancy](08.5-multi-tenancy.md)** → Isolate teams in shared cluster

---

## 🤔 Common Questions

**Q: Linkerd vs. Istio?**  
A: **Linkerd**: Simple, fast (Rust proxy), 80% of features. **Istio**: Complex, slower (Envoy), 100% features. Start with Linkerd. Switch to Istio if you need advanced features (multi-cluster mesh, rate limiting per user).

**Q: Performance overhead of service mesh?**  
A: **Linkerd**: 1-5ms latency, 5-10% CPU overhead. **Istio**: 5-15ms latency, 10-20% CPU overhead. Negligible for most apps.

**Q: Can I use service mesh with Ingress?**  
A: Yes! Ingress → API Gateway → Service Mesh. Ingress handles external traffic, service mesh handles internal traffic.

**Q: What about API Gateway vs. Service Mesh?**  
A: **API Gateway** (Kong, Ambassador): External API management (rate limiting, auth, API keys). **Service Mesh**: Internal service-to-service (mTLS, retries, circuit breakers). Use both!

**Q: How do I debug "connection refused" in mesh?**  
A: Check: (1) ServiceProfile allows route, (2) mTLS policy (default-inbound-policy), (3) NetworkPolicy (if enabled), (4) `linkerd viz tap` to see live traffic.

**Q: Can I gradually adopt service mesh?**  
A: Yes! Enable mesh per-namespace (`kubectl annotate namespace <name> linkerd.io/inject=enabled`). Start with dev/staging, then production.

**Q: What about service mesh for serverless (Lambda, Cloud Run)?**  
A: Service mesh is for Kubernetes only. For serverless, use **API Gateway + distributed tracing** (X-Ray, Cloud Trace).

---

## 🧹 Cleanup

```bash
# Delete microservices namespace
kubectl delete namespace microservices

# Uninstall Linkerd Viz
linkerd viz uninstall | kubectl delete -f -

# Uninstall Linkerd Jaeger
linkerd jaeger uninstall | kubectl delete -f -

# Uninstall Linkerd control plane
linkerd uninstall | kubectl delete -f -
```

---

**🎉 Congrats!** You can now manage Netflix-scale microservices. Time to architect like the pros! 🕸️
