# Lab 11.5: Disaster Recovery Drills üö®

**‚è± Time**: 90 minutes  
**üéØ Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê Expert  
**üìã Prerequisites**: Labs 3, 11, 12 complete + cluster admin access

> **Note for Windows Users**: This lab uses Linux-based commands for etcd and control plane access. Windows users should use **WSL2** (Ubuntu on Windows) or connect to a Linux control plane node via SSH. Most commands work identically in WSL2, and `kubectl` commands work from PowerShell/cmd as well.

---

## üéØ Objective

**The Scenario**: Your production Kubernetes cluster just suffered a catastrophic failure. The control plane is gone, etcd is corrupted, and your CEO is asking, *"When will we be back online?"*

**This Lab**: Practice 4 types of disaster recovery under time pressure. You'll learn:
1. **etcd backup/restore** (recover cluster state)
2. **Velero application backups** (recover workloads + data)
3. **Cross-cluster migration** (move to new cluster with zero downtime)
4. **RTO/RPO planning** (define acceptable downtime and data loss)

**Why This Matters**: Datadog lost 7 hours of customer data in 2019 because they didn't test backups. GitHub went offline for 24 hours due to split-brain. **Don't be them.**

---

## üìö Key Concepts

### Recovery Time Objective (RTO)
**Definition**: Maximum acceptable downtime before business impact.

**Examples**:
- **E-commerce site**: RTO = 15 minutes (every minute offline = $10K lost revenue)
- **Internal tool**: RTO = 4 hours (annoying but not catastrophic)
- **Batch processing**: RTO = 24 hours (can delay overnight jobs)

**Kubernetes Implication**: Your backup strategy must support your RTO. Daily backups won't work for 15-minute RTO.

### Recovery Point Objective (RPO)
**Definition**: Maximum acceptable data loss (measured in time).

**Examples**:
- **Financial transactions**: RPO = 0 seconds (zero data loss allowed)
- **Logs/analytics**: RPO = 1 hour (can lose last hour of logs)
- **User uploads**: RPO = 5 minutes (last 5 minutes of uploads lost)

**Kubernetes Implication**: Backup frequency must match RPO. 5-minute RPO needs backups every 5 minutes.

### Disaster Types

| Disaster | Impact | Recovery Method |
|----------|--------|-----------------|
| **Pod crash** | Single app down | Self-healing (Deployment recreates pod) |
| **Node failure** | All pods on node lost | Kubernetes reschedules to healthy nodes |
| **etcd corruption** | Cluster control plane dead | Restore from etcd backup |
| **Entire cluster lost** | Everything gone | Restore to new cluster (Velero) |
| **Data center fire** | Physical infrastructure destroyed | Failover to DR region |

**This lab focuses on**: etcd corruption + entire cluster loss (worst-case scenarios).

---

## üõ†Ô∏è Setup: Deploy Production-Like App

We'll use a stateful e-commerce app (frontend + backend + database) with real data.

```bash
# Create namespace
kubectl create namespace ecommerce-prod

# Deploy PostgreSQL with data
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: ecommerce-prod
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: ecommerce-prod
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_DB
          value: ecomdb
        - name: POSTGRES_USER
          value: ecomuser
        - name: POSTGRES_PASSWORD
          value: productionPassword123
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: ecommerce-prod
spec:
  clusterIP: None
  selector:
    app: postgres
  ports:
  - port: 5432
EOF

# Wait for PostgreSQL to be ready
kubectl wait --for=condition=ready pod -l app=postgres -n ecommerce-prod --timeout=120s

# Seed database with sample data
kubectl exec -n ecommerce-prod postgres-0 -- psql -U ecomuser -d ecomdb -c "
CREATE TABLE products (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100),
  price DECIMAL(10,2),
  created_at TIMESTAMP DEFAULT NOW()
);

INSERT INTO products (name, price) VALUES
  ('Laptop', 999.99),
  ('Mouse', 29.99),
  ('Keyboard', 79.99),
  ('Monitor', 299.99),
  ('Webcam', 59.99);
"

# Deploy backend API
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: ecommerce-prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: temitayocharles/ecommerce-backend:latest
        env:
        - name: DATABASE_URL
          value: postgres://ecomuser:productionPassword123@postgres:5432/ecomdb
        ports:
        - containerPort: 3001
---
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: ecommerce-prod
spec:
  selector:
    app: backend
  ports:
  - port: 3001
    targetPort: 3001
EOF

# Verify app is working
kubectl get pods -n ecommerce-prod
# All pods should be Running

# Test API (should return 5 products)
kubectl run curl-test --image=curlimages/curl -i --rm --restart=Never -n ecommerce-prod -- \
  curl -s http://backend:3001/api/products | jq length
# Expected: 5
```

**Setup Complete!** You now have a production-like app with persistent data.

---

## üî• Drill #1: etcd Backup & Restore (30 min)

### Scenario
Your cluster's etcd database just corrupted (disk failure). You need to restore from backup **NOW**.

### Step 1: Create etcd Backup (Before Disaster)

```bash
# Find etcd pod (control plane)
kubectl get pods -n kube-system | grep etcd

# Backup etcd (requires admin access to etcd pod)
# For managed K8s (EKS, GKE, AKS): Cloud provider handles this automatically
# For self-managed (kubeadm, k3s): You must backup manually

# Option A: Backup from etcd pod (kubeadm clusters)
kubectl exec -n kube-system etcd-$(hostname) -- sh -c \
  "ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key"

# Copy backup to local machine
kubectl cp kube-system/etcd-$(hostname):/tmp/etcd-backup.db ./etcd-backup-$(date +%Y%m%d-%H%M%S).db

# Option B: Backup from master node (SSH access)
# (If you have SSH access to control plane node)
sudo ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-$(date +%Y%m%d-%H%M%S).db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Verify backup integrity
ETCDCTL_API=3 etcdctl snapshot status etcd-backup-*.db --write-out=table
# Should show snapshot metadata (hash, revision, total keys)
```

**Backup Frequency Best Practices**:
- **RTO < 1 hour**: Backup every 15 minutes (CronJob)
- **RTO 1-4 hours**: Backup every hour
- **RTO > 4 hours**: Backup every 4-6 hours

### Step 2: Simulate Disaster (Delete Critical Resources)

```bash
# Simulate etcd corruption by deleting our app (will pretend etcd is corrupt)
kubectl delete namespace ecommerce-prod

# Verify it's gone
kubectl get all -n ecommerce-prod
# Expected: "No resources found" (disaster!)

# Check products are gone
kubectl run curl-test --image=curlimages/curl -i --rm --restart=Never -- \
  curl -s http://backend.ecommerce-prod:3001/api/products
# Expected: Connection refused (namespace deleted)
```

**üî• Disaster declared!** Your app is gone. CEO is calling.

### Step 3: Restore from etcd Backup

```bash
# STOP: In production, you'd restore to a NEW cluster (not the same cluster)
# This is because etcd restore requires stopping the entire control plane

# For this lab (simulated disaster), we'll just redeploy the app
# In real life, you'd:
# 1. Stop kube-apiserver, kube-controller-manager, kube-scheduler
# 2. Delete old etcd data: rm -rf /var/lib/etcd/*
# 3. Restore snapshot: etcdctl snapshot restore etcd-backup.db --data-dir=/var/lib/etcd
# 4. Restart control plane components

# Since we can't safely do this in a running cluster, we'll simulate by redeploying:
# (In production, etcd restore recovers ALL cluster state, including our namespace)

# Pretend etcd is restored (recreate namespace)
kubectl create namespace ecommerce-prod

# Redeploy app (in real scenario, etcd restore would bring this back)
kubectl apply -f - <<EOF
# (Same YAML as Setup section - PostgreSQL + backend)
# ... (truncated for brevity, use same manifests from Setup)
EOF

# Verify recovery
kubectl get pods -n ecommerce-prod
# All pods should be Running

# Check data is restored
kubectl exec -n ecommerce-prod postgres-0 -- psql -U ecomuser -d ecomdb -c \
  "SELECT COUNT(*) FROM products;"
# Expected: 5 (data restored!)
```

**‚è± Recovery Time**: ~10-15 minutes (acceptable for most non-critical apps)

### Step 4: Automate Backups (Production Setup)

```bash
# Create CronJob for automatic etcd backups
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  schedule: "*/15 * * * *"  # Every 15 minutes
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          hostNetwork: true
          containers:
          - name: backup
            image: bitnami/etcd:3.5
            command:
            - /bin/sh
            - -c
            - |
              ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-\$(date +%Y%m%d-%H%M%S).db \\
                --endpoints=https://127.0.0.1:2379 \\
                --cacert=/etc/kubernetes/pki/etcd/ca.crt \\
                --cert=/etc/kubernetes/pki/etcd/server.crt \\
                --key=/etc/kubernetes/pki/etcd/server.key
              
              # Upload to S3 (requires AWS CLI + IAM role)
              # aws s3 cp /backup/etcd-*.db s3://my-etcd-backups/
              
              # Keep only last 7 days of backups
              find /backup -name "etcd-*.db" -mtime +7 -delete
            volumeMounts:
            - name: etcd-certs
              mountPath: /etc/kubernetes/pki/etcd
              readOnly: true
            - name: backup-volume
              mountPath: /backup
          restartPolicy: OnFailure
          volumes:
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/pki/etcd
          - name: backup-volume
            hostPath:
              path: /var/backups/etcd  # Persistent volume in production!
EOF
```

**Key Takeaways**:
- ‚úÖ **Automate backups** (don't rely on manual snapshots)
- ‚úÖ **Test restores monthly** (backups are useless if untested)
- ‚úÖ **Store backups off-cluster** (S3, GCS, Azure Blob)
- ‚ö†Ô∏è **etcd restore = full cluster recreate** (plan for 15-30 min downtime)

---

## üéí Drill #2: Velero Application Backups (30 min)

**Why Velero?** etcd backups recover cluster state (deployments, services), but **NOT persistent data** (PVCs). Velero backs up both.

### Step 1: Install Velero

```bash
# Download Velero CLI
# macOS
brew install velero

# Linux
wget https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz
tar -xvf velero-v1.12.0-linux-amd64.tar.gz
sudo mv velero-v1.12.0-linux-amd64/velero /usr/local/bin/

# Windows (PowerShell as Administrator)
# Download from: https://github.com/vmware-tanzu/velero/releases
# Or use Chocolatey:
choco install velero
# Or use Scoop:
scoop install velero

# Install Velero in cluster (with MinIO for local testing)
kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/minio/00-minio-deployment.yaml

# Wait for MinIO to be ready
kubectl wait --for=condition=available --timeout=300s deployment/minio -n velero

# Get MinIO credentials
kubectl get secret -n velero minio -o jsonpath='{.data.access-key-id}' | base64 -d > /tmp/minio-access-key
kubectl get secret -n velero minio -o jsonpath='{.data.secret-access-key}' | base64 -d > /tmp/minio-secret-key

# Install Velero with MinIO backend
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.8.0 \
  --bucket velero \
  --secret-file /tmp/minio-credentials \
  --use-volume-snapshots=false \
  --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://minio.velero.svc:9000

# Verify installation
velero version
# Should show client + server versions
```

### Step 2: Create Application Backup

```bash
# Backup entire namespace (app + data)
velero backup create ecommerce-backup-$(date +%Y%m%d-%H%M%S) \
  --include-namespaces ecommerce-prod \
  --wait

# Check backup status
velero backup describe ecommerce-backup-*

# Expected output:
# Phase: Completed
# Backup Format Version: 1
# Total items to be backed up: 15  (deployments, services, PVCs, secrets, etc.)
# Items backed up: 15

# List all backups
velero backup get
# Should show your backup with "Completed" status
```

### Step 3: Simulate Total Cluster Loss

```bash
# Delete EVERYTHING (simulates cluster failure)
kubectl delete namespace ecommerce-prod

# Verify total loss
kubectl get all -n ecommerce-prod
# Expected: "Error from server (NotFound): namespaces 'ecommerce-prod' not found"

# GONE! All deployments, PVCs, data... vanished.
```

**üî• Disaster scenario**: Your cluster's entire control plane is dead. You've provisioned a NEW cluster and need to restore workloads + data.

### Step 4: Restore from Velero Backup

```bash
# Restore entire application (will recreate namespace, PVCs, data)
velero restore create --from-backup ecommerce-backup-* --wait

# Watch restore progress
kubectl get pods -n ecommerce-prod --watch

# Verify all resources are restored
kubectl get all -n ecommerce-prod

# Check database data is intact
kubectl exec -n ecommerce-prod postgres-0 -- psql -U ecomuser -d ecomdb -c \
  "SELECT * FROM products;"

# Expected: All 5 products are back!
# | id | name     | price  | created_at          |
# |----|----------|--------|---------------------|
# | 1  | Laptop   | 999.99 | 2025-10-20 12:00:00 |
# | 2  | Mouse    | 29.99  | 2025-10-20 12:00:00 |
# ... (all original data restored!)
```

**‚è± Recovery Time**: ~5-10 minutes (faster than etcd restore!)

**üéâ Success!** Your app + data is back from the dead.

### Step 5: Scheduled Backups (Production Setup)

```bash
# Create backup schedule (every 6 hours)
velero schedule create ecommerce-6h \
  --schedule="0 */6 * * *" \
  --include-namespaces ecommerce-prod \
  --ttl 168h  # Keep backups for 7 days

# Create daily backups (for long-term retention)
velero schedule create ecommerce-daily \
  --schedule="@daily" \
  --include-namespaces ecommerce-prod \
  --ttl 720h  # Keep for 30 days

# List schedules
velero schedule get

# Manual backup before risky changes (e.g., database migration)
velero backup create pre-migration-backup --include-namespaces ecommerce-prod
```

**Key Takeaways**:
- ‚úÖ **Velero backs up PVCs + cluster resources** (etcd only backs up cluster state)
- ‚úÖ **Restore to different cluster** (disaster recovery to new region)
- ‚úÖ **Schedule backups based on RPO** (6-hour RPO = backup every 6 hours)
- ‚úÖ **Test restores monthly** (automated restore drills)

---

## üåç Drill #3: Cross-Cluster Migration (20 min)

**Scenario**: Your current cluster is deprecated (old K8s version, wrong cloud region, etc.). Migrate to new cluster with **zero downtime**.

### Step 1: Provision "New" Cluster

```bash
# For this lab, we'll simulate by using a different namespace
# In production, this would be a separate cluster in a new region

# Create "new cluster" namespace
kubectl create namespace ecommerce-prod-new

# Label old namespace (for clarity)
kubectl label namespace ecommerce-prod migration=source
kubectl label namespace ecommerce-prod-new migration=target
```

### Step 2: Backup from Old Cluster

```bash
# Create backup from source cluster
velero backup create migration-backup \
  --include-namespaces ecommerce-prod \
  --wait

# Verify backup completed
velero backup describe migration-backup
```

### Step 3: Restore to New Cluster

```bash
# Restore to new namespace (simulates new cluster)
velero restore create migration-restore \
  --from-backup migration-backup \
  --namespace-mappings ecommerce-prod:ecommerce-prod-new \
  --wait

# Verify restoration in new namespace
kubectl get pods -n ecommerce-prod-new

# Test new cluster app
kubectl run curl-test --image=curlimages/curl -i --rm --restart=Never -n ecommerce-prod-new -- \
  curl -s http://backend.ecommerce-prod-new:3001/api/products | jq

# Expected: Same 5 products (data migrated!)
```

### Step 4: Cutover Traffic (Zero-Downtime)

```bash
# In production, you'd:
# 1. Update DNS to point to new cluster's LoadBalancer
# 2. Use Global Load Balancer (AWS Route53, GCP Cloud DNS) for gradual traffic shift
# 3. Monitor error rates during cutover

# For this lab, we'll use Ingress to shift traffic
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: migration-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"  # 10% traffic to new cluster
spec:
  rules:
  - host: ecommerce.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend
            port:
              number: 3001
---
# Gradually increase weight: 10% ‚Üí 50% ‚Üí 100%
# Monitor error rates at each step
# If errors spike, rollback to old cluster
EOF
```

### Step 5: Decommission Old Cluster

```bash
# After 24 hours of successful traffic on new cluster:
# Delete old namespace (simulates deleting old cluster)
kubectl delete namespace ecommerce-prod

# Verify new cluster is handling 100% traffic
kubectl get pods -n ecommerce-prod-new
# All pods healthy
```

**‚è± Migration Time**: 0 seconds downtime (users never noticed!)

**Key Takeaways**:
- ‚úÖ **Velero enables cross-cluster migration** (backup ‚Üí restore to new cluster)
- ‚úÖ **Gradual traffic shift** (10% ‚Üí 50% ‚Üí 100%, monitor at each step)
- ‚úÖ **Keep old cluster running 24 hours** (easy rollback if issues)
- ‚úÖ **Test new cluster thoroughly** before cutover (performance, data integrity)

---

## üìä Drill #4: RTO/RPO Planning (10 min)

**Exercise**: Define recovery objectives for different app tiers.

### Template: RTO/RPO Matrix

| Application | Business Impact | RTO | RPO | Backup Strategy |
|-------------|-----------------|-----|-----|-----------------|
| **E-commerce frontend** | $10K/min revenue loss | 5 min | 0 sec | Multi-region active-active |
| **E-commerce database** | Can't process orders | 15 min | 5 min | Velero every 5 min + DB replication |
| **Analytics dashboard** | Low (internal only) | 4 hours | 1 hour | Velero every 6 hours |
| **Batch jobs** | Can delay 24 hours | 24 hours | 24 hours | Daily Velero backups |

### Your Turn: Calculate RTO/RPO

**Scenario**: Your company's Kubernetes cluster hosts 4 apps. Define RTO/RPO for each:

1. **Payment API** (processes credit card transactions)
   - RTO: _____ (How long can payments be down?)
   - RPO: _____ (How much transaction data can you lose?)
   - Backup strategy: _____

2. **User profile service** (read-only user data)
   - RTO: _____
   - RPO: _____
   - Backup strategy: _____

3. **Log aggregation** (collects application logs)
   - RTO: _____
   - RPO: _____
   - Backup strategy: _____

4. **ML training pipeline** (runs overnight)
   - RTO: _____
   - RPO: _____
   - Backup strategy: _____

**Sample Answers**:
1. **Payment API**: RTO=1 min, RPO=0 sec (active-active multi-region + synchronous DB replication)
2. **User profiles**: RTO=30 min, RPO=15 min (Velero every 15 min + read replicas)
3. **Log aggregation**: RTO=2 hours, RPO=1 hour (Velero hourly, logs can be reprocessed)
4. **ML training**: RTO=24 hours, RPO=24 hours (daily backups, can rerun jobs)

---

## üéØ Final Challenge: Disaster Recovery Drill

**Time Limit**: 20 minutes  
**Scenario**: It's 3 AM. AWS just called‚Äîyour entire us-east-1 region is down (massive outage). You need to failover to us-west-2 **NOW**.

### Your Tasks:
1. ‚úÖ Create Velero backup of production namespace (5 min)
2. ‚úÖ Restore to "new region" namespace (5 min)
3. ‚úÖ Verify all data is intact (3 min)
4. ‚úÖ Update DNS to point to new region (2 min)
5. ‚úÖ Write incident report (5 min):
   - What caused the failure?
   - How long was downtime?
   - What data was lost (if any)?
   - How could this be prevented?

**Success Criteria**:
- Total recovery time < 20 minutes
- Zero data loss (all products still in database)
- New cluster serves traffic successfully

**Go!** ‚è±Ô∏è

---

## üßπ Cleanup

```bash
# Delete test namespaces
kubectl delete namespace ecommerce-prod ecommerce-prod-new

# Uninstall Velero (optional)
velero uninstall

# Delete backups
velero backup delete --all --confirm
```

---

## üéì Key Takeaways

### What You Learned
1. **etcd backups** recover cluster state (deployments, services, RBAC)
2. **Velero backups** recover cluster state + persistent data (PVCs)
3. **Cross-cluster migration** enables zero-downtime moves (region migration, cluster upgrades)
4. **RTO/RPO planning** dictates backup frequency and disaster recovery strategy

### Backup Strategy Comparison

| Method | What's Backed Up | Recovery Time | Use Case |
|--------|------------------|---------------|----------|
| **etcd snapshot** | Cluster state only (no PVC data) | 15-30 min | Cluster config recovery |
| **Velero** | Cluster state + PVC data | 5-15 min | Full application recovery |
| **Database replication** | Database data only | 0 min (instant failover) | Zero-downtime DR |
| **Multi-region active-active** | N/A (no backup, always running) | 0 min | Mission-critical apps |

### Production Checklist
- [ ] Automate etcd backups (CronJob every 15-60 min)
- [ ] Automate Velero backups (schedule based on RPO)
- [ ] Store backups off-cluster (S3, GCS, Azure Blob)
- [ ] Test restores monthly (automated drill)
- [ ] Document RTO/RPO for each app tier
- [ ] Practice disaster drills quarterly (don't wait for real disaster)
- [ ] Monitor backup success (Prometheus alerts if backup fails)

---

## üìö What's Next?

- **[Lab 12: External Secrets](12-external-secrets.md)** ‚Üí Secure secrets management (prevent Capital One breach)
- **[Lab 12.5: Multi-Cloud Secrets](12.5-multi-cloud-secrets.md)** ‚Üí AWS Secrets Manager, GCP Secret Manager integration *(coming soon)*
- **[Production War Stories](../docs/reference/production-war-stories.md)** ‚Üí Real disasters (GitHub, Salesforce, Datadog)

---

## ü§î Common Questions

**Q: Should I backup etcd or use Velero?**  
A: **Both!** etcd backups are fast (recover cluster state quickly). Velero backups include data (recover applications fully). Use etcd for quick cluster recovery, Velero for full disaster recovery.

**Q: How often should I backup?**  
A: **Depends on RPO**. Financial apps (RPO=0): Continuous replication. Most apps (RPO=5-15 min): Velero every 15 min. Internal tools (RPO=4 hours): Hourly backups.

**Q: Can I restore to a different K8s version?**  
A: **Yes, with caution**. Velero supports restoring to newer K8s versions (e.g., 1.25 ‚Üí 1.28), but not older. Test compatibility first. API versions may change (e.g., `extensions/v1beta1` ‚Üí `apps/v1`).

**Q: What if my cluster is on EKS/GKE/AKS?**  
A: **Cloud providers auto-backup etcd** (you can't access it directly). Use Velero for application backups. Managed K8s has high availability (3+ control plane nodes), but Velero is still critical for data recovery.

**Q: How do I test backups without disrupting production?**  
A: **Restore to test cluster**. Create a separate "dr-test" cluster, restore backups there monthly. Verify data integrity without touching production.

---

**üéâ Congrats!** You can now recover from worst-case disasters. Sleep better knowing your cluster is resilient! üöÄ

**Remember**: Backups are useless if untested. Schedule monthly disaster recovery drills.
